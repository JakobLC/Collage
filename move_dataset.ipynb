{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as nd\n",
    "import sys\n",
    "import cv2\n",
    "import clip\n",
    "import torchvision\n",
    "import os\n",
    "from copy import copy,deepcopy\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib.lines import Line2D\n",
    "import jlc\n",
    "import clip\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "ROOT = \"C:/Users/Jakob/Desktop/DTU/Computational_Photography/\"\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "from collage_functions import (CollageTransformer,save_model,load_model,\n",
    "                               validate,train,loss_with_ignore_index,\n",
    "                               plot_loss_dict,CollageMaker,\n",
    "                               CollageDataset,custom_collate_with_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CollageDatasetSaver(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 collage_sizes = list(range(1,8)),\n",
    "                 candidate_sizes = [8],\n",
    "                 positive_candidate_prob = 0.5,\n",
    "                 collage_type_prob = [0.25,0.25,0.25,0.25],\n",
    "                 normalize = True,\n",
    "                 padding_idx = -1,\n",
    "                 clip_dim = 768,\n",
    "                 small_group_strategy=\"concat\",\n",
    "                 min_group_length = None,\n",
    "                 filter_positives = True,\n",
    "                 split_start_and_stop = [0,1],\n",
    "                 allow_dataset_mixing = False,\n",
    "                 CNN = False,\n",
    "                 cnn_reshape = [224,224,3]):\n",
    "        \"\"\"Dataset for training a CollageTransformer\n",
    "\n",
    "        Args:\n",
    "            collage_sizes (list, optional): array to sample the collage size from uniformly. Defaults to list(range(1,8)).\n",
    "            candidate_sizes (list, optional): array to sample the candidate size from uniformly. Defaults to [8].\n",
    "            positive_candidate_prob (float, optional): Probability of sampling positive candidates for the collage. Defaults to 0.5.\n",
    "            collage_type_prob (list, optional): length 4 array or list with probabilities for different collage types in the order [\"font\",\"letter\",\"same_prompt\",\"1k\"] for datasets [Dafont-free,Dafont-free,Simulacra,LAION-Aesthetics6.5+]. Defaults to [0.25,0.25,0.5,0].\n",
    "            normalize (bool, optional): Should the clip vectors be L2-normalized. Defaults to True.\n",
    "            padding_idx (int, optional): padding index used for the torch.tensor entries which are empty since the collage was smaller than the maximum size. Defaults to -1.\n",
    "            clip_dim (int, optional): Dimension of clip vectors. Defaults to 768.\n",
    "            small_group_strategy ([\"concat\",\"ignore\"], optional): How to deal with small groups (collages). \"concat\" combines small groups with other groups. \"ignore\" only uses single groups - however they can be very small which usually ends in the same images being sampled many times. Defaults to \"concat\".\n",
    "            min_group_length (int, optional): Minimum size which groups should have to be included in the dataset. Defaults to None.\n",
    "            filter_positives (bool, optional): Should positives which are already in the collage not be included in candidates. Defaults to True.\n",
    "            split_start_and_stop (list, optional): dataset starting and stopping point in terms of ratio of indices to use for dataloader. Defaults to [0,1].\n",
    "            allow_dataset_mixing (bool, optional): should the model show candidates from different datasets than the sampled used for the collage. Defaults to False.\n",
    "            CNN (bool, optional): Is a CNN being used (instead of CLIP) to embed images to vectors. Defaults to False.\n",
    "            cnn_reshape (list, optional): Reshape size for inputs of CNN if a CNN is used for embedding vectors. Defaults to [224,224,3].\n",
    "        \"\"\"\n",
    "        assert small_group_strategy in [\"concat\",\"ignore\"]\n",
    "        self.CNN = CNN\n",
    "        self.allow_dataset_mixing = allow_dataset_mixing\n",
    "        self.split_start_and_stop = split_start_and_stop\n",
    "        self.small_group_strategy = small_group_strategy\n",
    "        self.filter_positives = filter_positives\n",
    "        self.clip_dim = clip_dim\n",
    "        self.positive_candidate_prob = positive_candidate_prob\n",
    "        self.normalize = normalize\n",
    "        self.padding_idx = padding_idx\n",
    "        self.cnn_reshape = cnn_reshape\n",
    "\n",
    "        self.dataset_types = [0,0,1,2]\n",
    "        self.collage_types = [\"font\",\"letter\",\"same_prompt\",\"1k\"]\n",
    "\n",
    "        self.dataset_lengths = [0,0,0]\n",
    "        self.groups = []\n",
    "        self.group_types = []\n",
    "        self.group_lengths = []\n",
    "        self.image_names = []\n",
    "        self.collage_type_prob = np.array(collage_type_prob)\n",
    "        self.collage_type_prob = self.collage_type_prob/(self.collage_type_prob.sum())\n",
    "        self.collage_sizes = collage_sizes\n",
    "        max_collage_size = max(collage_sizes)\n",
    "        self.candidate_sizes = candidate_sizes\n",
    "        max_candidate_size = max(candidate_sizes)\n",
    "        self.min_group_length = min_group_length\n",
    "        if self.min_group_length is None:\n",
    "            self.min_group_length = max_collage_size//2\n",
    "            \n",
    "        self.min_group_size = int(max_collage_size+self.positive_candidate_prob*max_candidate_size)\n",
    "        self.use_dataset = [False,False,False]\n",
    "        self.use_groups = []\n",
    "        self.group_names = []\n",
    "        self.group_type_lengths = []\n",
    "        self.clip_matrix = torch.zeros(0,clip_dim)\n",
    "        self.num_groups = 0\n",
    "        self.translation = 0\n",
    "        for i in range(len(self.collage_types)):\n",
    "            if self.collage_type_prob[i]>0:\n",
    "                ii = self.dataset_types[i]\n",
    "                if not self.use_dataset[ii]:\n",
    "                    self.translation = sum(self.dataset_lengths)\n",
    "                    self.use_dataset[ii] = True\n",
    "                    clip_matrix, names = self.get_clip_and_names(self.dataset_types[i]) \n",
    "                    self.image_names.extend(names)\n",
    "                    self.dataset_lengths[ii] = len(names)\n",
    "                    self.clip_matrix = torch.cat((self.clip_matrix,clip_matrix),axis=0)\n",
    "                groups, group_names = self.get_groups(names,i,translation=self.translation)\n",
    "                self.group_names.extend(group_names)\n",
    "                self.num_groups += len(groups)\n",
    "                self.groups.extend(groups)\n",
    "                self.group_types.extend([i for _ in range(len(groups))])\n",
    "                self.group_lengths.extend([len(g) for g in groups])\n",
    "                self.group_type_lengths.append(len(groups))\n",
    "            else:\n",
    "                self.group_type_lengths.append(0)\n",
    "\n",
    "        self.group_lengths = np.array(self.group_lengths)\n",
    "        self.group_types = np.array(self.group_types)\n",
    "\n",
    "        if normalize:\n",
    "            self.clip_matrix = torch.nn.functional.normalize(self.clip_matrix,dim=1)\n",
    "\n",
    "    def save_dataset(self,dataset_idx):\n",
    "        i = dataset_idx\n",
    "        start = sum(self.dataset_lengths[:i])\n",
    "        stop = sum(self.dataset_lengths[:i+1])\n",
    "        dataset_slice = slice(start,stop,None)\n",
    "        clip_matrix, image_names = self.get_clip_and_names(dataset_idx)\n",
    "        save_dict = {\"CLIP_matrix\": clip_matrix,\n",
    "                     \"image_names\": image_names}\n",
    "        save0 = ROOT+\"Deliverables/test_set_data_compressed/\"\n",
    "        if i==0:\n",
    "            save_path = ROOT+\"Deliverables/test_set_data_compressed/Dafonts/\"\n",
    "            dict_save_path = save0+\"CLIP_fonts.pth\"\n",
    "        elif i==1:\n",
    "            save_path = ROOT+\"Deliverables/test_set_data_compressed/Simulacra/\"\n",
    "            dict_save_path = save0+\"CLIP_sim.pth\"\n",
    "        elif i==2:\n",
    "            save_path = ROOT+\"Deliverables/test_set_data_compressed/LAION_aesthetics_6dot5/\"\n",
    "            dict_save_path = save0+\"CLIP_laion.pth\"\n",
    "        torch.save(save_dict,dict_save_path)\n",
    "        print(\"saved CLIP for dataset_idx: \",i,clip_matrix.shape)\n",
    "        for idx in tqdm.tqdm(range(start,stop)):\n",
    "            if 0<=idx and idx<self.dataset_lengths[0]:\n",
    "                img_path = ROOT+\"d0/dafonts-free-v1/fonts_images/\"+self.image_names[idx]+\".png\"\n",
    "                img_path_new = save_path+self.image_names[idx]+\".png\"\n",
    "                directory = save_path+self.image_names[idx][:self.image_names[idx].find(\"\\\\\")]\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "            elif sum(self.dataset_lengths[:1])<=idx and idx<sum(self.dataset_lengths[:2]):\n",
    "                img_path = ROOT+\"d1/sac/\"+self.image_names[idx]+\".png\"\n",
    "                img_path_new = save_path+self.image_names[idx]+\".jpg\"\n",
    "            elif sum(self.dataset_lengths[:2])<=idx and idx<sum(self.dataset_lengths[:3]):\n",
    "                img_path = ROOT+\"d2/\"+self.image_names[idx]+\".jpg\"\n",
    "                img_path_new = save_path+self.image_names[idx][self.image_names[idx].find(\"\\\\\")+2:]+\".jpg\"\n",
    "            pil_image = Image.open(img_path)\n",
    "            pil_image.save(img_path_new)\n",
    "\n",
    "    def get_clip_and_names(self,dataset_idx):\n",
    "        \"\"\"loads CLIP matrix and names which define groups in datasets\n",
    "\n",
    "        Args:\n",
    "            dataset_idx (int): index of the dataset\n",
    "\n",
    "        Returns:\n",
    "            clip_matrix,names: clip matrix and names of images as a list in the same order as the clip matrix\n",
    "        \"\"\"\n",
    "        if dataset_idx==0:\n",
    "            loaded = torch.load(ROOT+\"d0/CLIP_fonts.pth\")\n",
    "        elif dataset_idx==1:\n",
    "            loaded = torch.load(ROOT+\"d1/CLIP_sim.pth\")\n",
    "        elif dataset_idx==2:\n",
    "            loaded = torch.load(ROOT+\"d2/CLIP_laion.pth\")\n",
    "        n = len(loaded[\"CLIP_matrix\"])\n",
    "        start = max(0,np.floor(self.split_start_and_stop[0]*n).astype(int))\n",
    "        stop = min(n,np.ceil(self.split_start_and_stop[1]*n).astype(int))\n",
    "        clip_matrix = loaded[\"CLIP_matrix\"][start:stop].clone()\n",
    "        names = loaded[\"image_names\"][start:stop]\n",
    "        return clip_matrix, names\n",
    "\n",
    "    def get_groups(self,names,group_idx,translation):\n",
    "        \"\"\"returns a set of groups (collages) from a given list of names\n",
    "\n",
    "        Args:\n",
    "            names (list): list of image names\n",
    "            group_idx (int): group index\n",
    "            translation (int): integer translation in terms if image indices, to make sure they are unique\n",
    "\n",
    "        Returns:\n",
    "            groups,group_names: list of groups and the name of each associated group\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        if group_idx==0: #font\n",
    "            group_names = np.array([n.split('\\\\')[0] for n in names])\n",
    "        elif group_idx==1: #letter\n",
    "            group_names = np.array([n.split('\\\\')[1] for n in names])\n",
    "        elif group_idx==2: #same_prompt\n",
    "            group_names = np.array([n.split('_')[0] for n in names])\n",
    "        elif group_idx==3:\n",
    "            group_names = np.tile(np.arange(10),(1,np.ceil(len(names)/10).astype(int)))[:len(names)].flatten()\n",
    "        uq,uq_inverse = np.unique(group_names,return_inverse=True)\n",
    "        groups = [[] for _ in range(len(uq))]\n",
    "        for i_sample,i_group in enumerate(uq_inverse):\n",
    "            groups[i_group].append(i_sample+translation)\n",
    "        \n",
    "        group_names = [group_names[j] for j in range(len(groups)) if len(groups[j])>=self.min_group_length]\n",
    "        groups = [g for g in groups if len(g)>=self.min_group_length]\n",
    "        return groups,group_names\n",
    "\n",
    "    def get_images(self,image_idx,reshape_size=None,return_torch=False,input_for_reshape=None):\n",
    "        \"\"\"return images from a list of image indices\n",
    "\n",
    "        Args:\n",
    "            image_idx (Union[int,list]): list of indices to get images for\n",
    "            reshape_size (list, optional): list,tuple or array to reshape images into. Defaults to None.\n",
    "            return_torch (bool, optional): should a torch tensor be returned. Defaults to False.\n",
    "            input_for_reshape (list, optional): list of images to reshape instead of loading images from image_idx. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            images: images\n",
    "        \"\"\"\n",
    "        if input_for_reshape is None:\n",
    "            if isinstance(image_idx,int):\n",
    "                image_idx = [image_idx]\n",
    "            images = []\n",
    "            for idx in image_idx:\n",
    "                if idx<0:\n",
    "                    continue\n",
    "                elif 0<=idx and idx<self.dataset_lengths[0]:\n",
    "                    img_path = ROOT+\"d0/dafonts-free-v1/fonts_images/\"+self.image_names[idx]+\".png\"\n",
    "                elif sum(self.dataset_lengths[:1])<=idx and idx<sum(self.dataset_lengths[:2]):\n",
    "                    img_path = ROOT+\"d1/sac/\"+self.image_names[idx]+\".png\"\n",
    "                elif sum(self.dataset_lengths[:2])<=idx and idx<sum(self.dataset_lengths[:3]):\n",
    "                    img_path = ROOT+\"d2/\"+self.image_names[idx]+\".jpg\"\n",
    "                pil_image = Image.open(img_path)\n",
    "                images.append(np.array(pil_image))\n",
    "        else:\n",
    "            images = input_for_reshape\n",
    "\n",
    "        if reshape_size is not None:\n",
    "            if isinstance(reshape_size,tuple):\n",
    "                reshape_size = list(reshape_size)\n",
    "            if len(reshape_size)<3:\n",
    "                reshape_size += [3]\n",
    "            images = [cv2.resize(im, reshape_size[:2], interpolation=cv2.INTER_LINEAR) for im in images]\n",
    "            for i in range(len(images)):                \n",
    "                im_size = images[i].shape\n",
    "                if len(im_size)==2:\n",
    "                    images[i] = images[i][:,:,None]\n",
    "                    im_size = images[i].shape\n",
    "                assert reshape_size[2] in [1,3]\n",
    "                if reshape_size[2]==3:\n",
    "                    if im_size[2]==1:\n",
    "                        images[i]=np.tile(images[i],(1,1,3))\n",
    "                    elif im_size[2]==4:\n",
    "                        images[i] = images[i][:3]\n",
    "                    else:\n",
    "                        assert im_size[2]==3\n",
    "                elif reshape_size[2]==1:\n",
    "                    images[i] = images[i].mean(2,keepdims=True)\n",
    "        if return_torch and (reshape_size is not None):\n",
    "            images = torch.stack([torch.from_numpy(im) for im in images],axis=0).permute((0,3,1,2))\n",
    "        elif return_torch and (reshape_size is None):\n",
    "            images = [torch.from_numpy(im) for im in images]\n",
    "        return images\n",
    "    \n",
    "    def sample_group_idx(self,group_type):\n",
    "        \"\"\"Fast sampling function for getting a group idx\n",
    "\n",
    "        Args:\n",
    "            group_type (int): index of group type to sample from\n",
    "\n",
    "        Returns:\n",
    "            idx: sampled group idx\n",
    "        \"\"\"\n",
    "        if group_type==0:\n",
    "            idx = np.random.choice(self.group_type_lengths[0])\n",
    "        elif group_type>0:\n",
    "            idx = np.random.choice(self.group_type_lengths[group_type])+sum(self.group_type_lengths[:group_type])\n",
    "        return idx\n",
    "    \n",
    "    def sample_negative_idx(self,group_type,group_indices):\n",
    "        \"\"\"Sample a negative index from a group type, but without sampling from indices in group_indices\n",
    "\n",
    "        Args:\n",
    "            group_type (int): group type index from which to sample from\n",
    "            group_indices (list): list of group indices which constitute the positive groups, and therefore not sampled\n",
    "\n",
    "        Returns:\n",
    "            idx: sampled negative index\n",
    "        \"\"\"\n",
    "        illegal_indices = sum([self.groups[id] for id in group_indices],[])\n",
    "        for _ in range(5):\n",
    "            if self.allow_dataset_mixing:\n",
    "                idx = np.random.choice(sum(self.dataset_lengths))\n",
    "            elif self.dataset_types[group_type]==0:\n",
    "                idx = np.random.choice(self.dataset_lengths[0])\n",
    "            else:# self.dataset_types[group_type]==1 or 2:\n",
    "                k = self.dataset_types[group_type]\n",
    "                idx = np.random.choice(self.dataset_lengths[k])+sum(self.dataset_lengths[:k])\n",
    "            if idx not in illegal_indices:\n",
    "                break\n",
    "        return idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(self.dataset_lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        max_collage_size = max(self.collage_sizes)\n",
    "        max_candidate_size = max(self.candidate_sizes)\n",
    "\n",
    "        n_collage = np.random.choice(self.collage_sizes)\n",
    "        n_candidates = np.random.choice(self.candidate_sizes)\n",
    "\n",
    "        info = {}\n",
    "        group_type = np.random.choice(len(self.collage_types),p=self.collage_type_prob)\n",
    "\n",
    "        info[\"group_type\"] = [group_type,self.collage_types[group_type]]\n",
    "\n",
    "        if self.small_group_strategy==\"concat\":\n",
    "            positives = []\n",
    "            group_indices = []\n",
    "            for _ in range(max_collage_size+max_candidate_size):\n",
    "                group_idx = self.sample_group_idx(group_type)\n",
    "                group_indices.append(group_idx)\n",
    "                positives.extend(self.groups[group_idx])\n",
    "\n",
    "                if len(positives)>self.min_group_size:\n",
    "                    break\n",
    "        elif self.small_group_strategy==\"ignore\":\n",
    "            group_idx = self.sample_group_idx(group_type)\n",
    "            positives = self.groups[group_idx]\n",
    "            group_indices = [group_idx]\n",
    "        \n",
    "        collage_idx = self.padding_idx*torch.ones(max_collage_size,dtype=int)\n",
    "        collage_idx[:n_collage] = torch.from_numpy(np.random.choice(positives, size=n_collage)).long()\n",
    "\n",
    "        if len(group_indices)>1:\n",
    "            group_indices_actually_used = []\n",
    "            for g_idx in group_indices:\n",
    "                if any([(g_i in collage_idx[:n_collage]) for g_i in self.groups[g_idx]]):\n",
    "                    group_indices_actually_used.append(g_idx)\n",
    "            positives = sum([self.groups[id] for id in group_indices_actually_used],[])\n",
    "            group_indices = group_indices_actually_used\n",
    "        \n",
    "        info[\"group_indices\"] = group_indices\n",
    "        info[\"group_names\"] = [self.group_names[g_idx] for g_idx in group_indices]\n",
    "\n",
    "\n",
    "\n",
    "        if self.filter_positives:\n",
    "            positives_tmp = [n for n in positives if n not in collage_idx]\n",
    "            if len(positives_tmp)>0:\n",
    "                positives = positives_tmp\n",
    "\n",
    "        label = self.padding_idx*torch.ones(max_candidate_size,dtype=torch.float32)\n",
    "        candidate_idx = self.padding_idx*torch.ones(max_candidate_size,dtype=int)\n",
    "        for i in range(n_candidates):\n",
    "            if np.random.rand()<self.positive_candidate_prob: \n",
    "                candidate_idx[i] = np.random.choice(positives)\n",
    "                label[i] = 1\n",
    "            else:\n",
    "                candidate_idx[i] = self.sample_negative_idx(group_type,group_indices)\n",
    "                label[i] = 0\n",
    "        \n",
    "        if self.CNN:\n",
    "            candidate = self.padding_idx*torch.ones(max_candidate_size,self.cnn_reshape[2],self.cnn_reshape[0],self.cnn_reshape[1],dtype=torch.float32)\n",
    "            collage = self.padding_idx*torch.ones(max_collage_size,self.cnn_reshape[2],self.cnn_reshape[0],self.cnn_reshape[1],dtype=torch.float32)\n",
    "            candidate[:n_candidates] = self.get_images(candidate_idx,reshape_size=self.cnn_reshape,return_torch=True)/255\n",
    "            collage[:n_collage] = self.get_images(collage_idx,reshape_size=self.cnn_reshape,return_torch=True)/255\n",
    "        else:\n",
    "            candidate = self.padding_idx*torch.ones(max_candidate_size,self.clip_dim,dtype=torch.float32)\n",
    "            collage = self.padding_idx*torch.ones(max_collage_size,self.clip_dim,dtype=torch.float32)\n",
    "            candidate[:n_candidates] = self.clip_matrix[candidate_idx[:n_candidates]]\n",
    "            collage[:n_collage] = self.clip_matrix[collage_idx[:n_collage]]\n",
    "\n",
    "        return collage, candidate, label, collage_idx, candidate_idx, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved CLIP for dataset_idx:  0 torch.Size([16233, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16233/16233 [00:26<00:00, 614.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved CLIP for dataset_idx:  1 torch.Size([5973, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5973/5973 [00:18<00:00, 327.51it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = CollageDatasetSaver(split_start_and_stop=[0.975,1.0])\n",
    "dataset.save_dataset(0)\n",
    "dataset.save_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved CLIP for dataset_idx:  2 torch.Size([1000, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 188.32it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = CollageDatasetSaver(split_start_and_stop=[0.9,1.0])\n",
    "dataset.save_dataset(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 768])\n"
     ]
    }
   ],
   "source": [
    "dataset = CollageDataset(collage_type_prob=[0,0,0,1])\n",
    "print(dataset.clip_matrix.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
